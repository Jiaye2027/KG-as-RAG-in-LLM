{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10693022,"sourceType":"datasetVersion","datasetId":6625612}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install langchain_openai langchain langchain_community faiss-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T19:55:25.204160Z","iopub.execute_input":"2025-02-23T19:55:25.204464Z","iopub.status.idle":"2025-02-23T19:55:38.806180Z","shell.execute_reply.started":"2025-02-23T19:55:25.204440Z","shell.execute_reply":"2025-02-23T19:55:38.805285Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"create history aware retriever","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport bs4\nfrom uuid import uuid4\nfrom langchain import hub\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader, WebBaseLoader\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.chains import RetrievalQA\nfrom langchain.schema import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain.memory import ChatMessageHistory\nfrom langchain_core.tools import Tool\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nprint('Import completed')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T19:56:08.311556Z","iopub.execute_input":"2025-02-23T19:56:08.311902Z","iopub.status.idle":"2025-02-23T19:56:10.933170Z","shell.execute_reply.started":"2025-02-23T19:56:08.311871Z","shell.execute_reply":"2025-02-23T19:56:10.932496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SETUPs\n\n# Set up API key\nOPENAI_API_KEY = \"sk...\"\n\nchat_prompt = \"\"\"You are a mathematical AI assistant. If it is a math related problem, use this response format:\n\n**Problem Analysis**\nAnalyze the promblem\n\n**Key Formulas**\nAll formulas that can be used to solve the problem\n- Formula 1\n- Formula 2\n...\n\n**Solution Steps**\nDetail explanation for each step\n1. step1\n2. step2\n...\n\n**Self-Check**\nCheck the intuition, formulas, solution steps. Try to prove your result is wrong. If the result is wrong, redo the problem. If the result is correct, prove it again.\n- Check 1\n- Check 2\n...\n\n**Final Answer**\n[Clear conclusion with boxed answer]\"\"\"\n\n# Load the agent prompt from LagnSmith\nagent_prompt = hub.pull(\"hwchase17/react-chat\") # a common agent, should consider create one for this specific task\n\nRAG_file_path = '/kaggle/input/probability-textbook/Probability.pdf'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T19:58:14.500819Z","iopub.execute_input":"2025-02-23T19:58:14.501097Z","iopub.status.idle":"2025-02-23T19:58:14.561558Z","shell.execute_reply.started":"2025-02-23T19:58:14.501077Z","shell.execute_reply":"2025-02-23T19:58:14.560796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\ndef defined_llm(OPENAI_API_KEY, chat_prompt=chat_prompt):\n    SESSION_ID = str(uuid4())\n    print(f\"Session ID: {SESSION_ID}\")\n    \n    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n    embedding_provider = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=800,\n        chunk_overlap=150,\n        separators=[r'\\n\\s*‚Ä¢', r'\\n\\d+\\.', r'\\n\\*', '\\n\\n'],\n        add_start_index=True\n    )\n    \n    vectorstore = FAISS.from_texts(\n        texts=[\"Math Knowledge Base Initialized\"], \n        embedding=embedding_provider,\n        metadatas=[{\"source\": \"system-init\"}]\n    )\n    retriever = vectorstore.as_retriever(\n        search_type=\"mmr\",\n        search_kwargs={\n            \"k\": 5,\n            \"score_threshold\": 0.7,\n            \"lambda_mult\": 0.5\n        }\n)\n\n    def format_docs(docs):\n        formatted = []\n        for i, doc in enumerate(docs):\n            # Handle Document objects\n            if hasattr(doc, 'metadata'):\n                source_type = doc.metadata.get('source_type', 'unknown')\n                source = doc.metadata.get('source_path', doc.metadata.get('source_url', 'unknown'))\n                content = doc.page_content\n            # Handle strings (fallback)\n            else:\n                source_type = \"unknown\"\n                source = \"unknown\"\n                content = str(doc)\n                \n            formatted.append(\n                f\"üìö Source {i+1} ({source_type}): {source}\\n\"\n                f\"{content[:500]}...\"\n            )\n        return \"\\n\\n\".join(formatted)\n\n    def debug_retrieval(query, retriever, top_k=3):\n        docs = retriever.invoke(query)\n        print(\"\\nüîç Retrieved Context Preview:\")\n        for i, doc in enumerate(docs[:top_k]):\n            print(f\"\\nüìÑ Document {i+1}:\")\n            print(f\"   Source: {doc.metadata.get('source_url', doc.metadata.get('source_path', 'unknown'))}\")\n            print(f\"   Content: {doc.page_content[:300]}...\")\n        return docs\n\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", chat_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"Context:\\n{context}\\n\\nQuestion: {input}\"),\n    ])\n\n    # Core processing chain\n    rag_chain = (\n        RunnablePassthrough.assign(\n            context=lambda x: format_docs(x[\"context\"]),\n            chat_history=lambda x: x[\"chat_history\"]\n        )\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n\n    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question:\n    1. DO NOT modify or rephrase the original question\n    2. Instead, add any relevant background information from the chat history as a prefix\n    3. Format the output as:\n       [Background Info (if any)]\n       Original Question: <exact original question>\"\"\"\n    \n    contextualize_q_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ])\n\n    def process_query(input_text, chat_history):\n        standalone_chain = contextualize_q_prompt | llm | StrOutputParser()\n        standalone_question = standalone_chain.invoke({\n            \"chat_history\": chat_history,\n            \"input\": input_text\n        })\n        \n        relevant_docs = retriever.invoke(standalone_question)\n        context = format_docs(relevant_docs)\n        debug_retrieval(standalone_question, retriever)\n        \n        response = rag_chain.invoke({\n            \"input\": input_text,\n            \"context\": context,\n            \"chat_history\": chat_history\n        })\n       \n        # Self-check verification\n        check_prompt = f\"\"\"Verify this solution contains:\n        1. Make sure the logic is correct. Check the method, formulas, each step.\n        2. Try to prove the response is incorrect. If it is incorrect, follow the logic and redo the problem.\n        3. Prove the response is correct.\n\n        If any issues are found, provide the correction.\n        If no issues are found, respond with \"VERIFIED\" and briefly explain why.\n        \n        Solution to verify:\n        {response}\n        \n        Missing/incorrect components:\"\"\"\n        \n        verification = rag_chain.invoke({\n            \"input\": check_prompt,\n            \"context\": context,\n            \"chat_history\": chat_history\n        })\n        \n        # If verification finds issues, get corrected solution\n        if \"VERIFIED\" not in verification:\n            corrected = rag_chain.invoke({\n                \"input\": f\"\"\"The previous solution had issues. Please provide a corrected solution that addresses these verification issues:{verification}\n                Original question: {input_text}\"\"\",\n                \"context\": context,\n                \"chat_history\": chat_history\n            })\n            return f\"{response}\\n\\n**Original Verification Feedback:**\\n{verification}\\n\\n**Corrected Response:**\\n{corrected}\"\n        \n        return f\"{response}\\n\\n**Verification:**\\n{verification}\"\n\n    # Memory\n    store = {}\n    def get_memory(session_id: str) -> ChatMessageHistory:\n        if session_id not in store:\n            store[session_id] = ChatMessageHistory(max_messages=20)\n        return store[session_id]\n\n    chat_agent = RunnableWithMessageHistory(\n        RunnablePassthrough.assign(\n            response=lambda x: process_query(x[\"input\"], x[\"chat_history\"])\n        ),\n        get_memory,\n        input_messages_key=\"input\",\n        history_messages_key=\"chat_history\",\n    )\n\n    def process_web_content(url):\n        try:\n            loader = WebBaseLoader(\n                web_paths=[url],\n                bs_kwargs=dict(\n                    parse_only=bs4.SoupStrainer(\n                        # Universal content detection\n                        ['article', 'main', 'div', 'section', 'content'],\n                        class_=lambda value: value and any(\n                            kw in value.lower()\n                            for kw in ['content', 'article', 'main', 'body', 'text']\n                        )\n                    )\n                )\n            )\n            docs = loader.load()\n            splits = text_splitter.split_documents(docs)\n            \n            for split in splits:\n                split.metadata.update({\n                    \"source_type\": \"web\",\n                    \"source_url\": url,\n                    \"content_type\": self_detect_content_type(split.page_content)\n                })\n            \n            vectorstore.add_documents(splits)\n            print(f\"\\nAI: Added {len(splits)} chunks from {url}\")\n            return True\n        except Exception as e:\n            print(f\"\\nAI: Error processing {url}: {str(e)}\")\n            return False\n\n    def self_detect_content_type(text):\n        math_keywords = ['theorem', 'formula', 'equation', 'proof', 'lemma']\n        if any(kw in text.lower() for kw in math_keywords):\n            return \"math\"\n        return \"general\"\n\n    # Interactive loop\n    print(\"Math Expert System - Type 'exit' to quit\")\n    print(\"Input formats:\")\n    print(\"- RAG_file_path=\\\"/path/to/file.pdf\\\" https://example.com Your question\")\n    print(\"- Include math formulas using $...$ notation\")\n    \n    url_pattern = re.compile(\n        r'(?:http|ftp)s?://(?:[A-Z0-9-]+\\.)+[A-Z]{2,}(?::\\d+)?(?:/[\\w\\-./?%&=]*)?', \n        re.IGNORECASE\n    )\n    \n    while True:\n        try:\n            user_input = input(\"\\nYou: \").strip()\n            if user_input.lower() in ['exit', 'quit']:\n                print(\"Goodbye!\")\n                break\n\n            # Process file paths\n            file_paths = re.findall(r'RAG_file_path=\"([^\"]+)\"', user_input)\n            for fp in file_paths:\n                try:\n                    if not os.path.exists(fp):\n                        raise FileNotFoundError(f\"File not found: {fp}\")\n                        \n                    loader = PyPDFLoader(fp) if fp.endswith('.pdf') else TextLoader(fp)\n                    docs = loader.load()\n                    splits = text_splitter.split_documents(docs)\n                    \n                    # Add metadata\n                    for split in splits:\n                        split.metadata.update({\n                            \"source_type\": \"file\",\n                            \"source_path\": fp\n                        })\n                    \n                    vectorstore.add_documents(splits)\n                    print(f\"\\nAI: Added {len(splits)} chunks from {os.path.basename(fp)}\")\n                    user_input = user_input.replace(f'RAG_file_path=\"{fp}\"', '').strip()\n                except Exception as e:\n                    print(f\"\\nAI: Error processing {fp}: {str(e)}\")\n\n            # Process URLs\n            urls = re.findall(r'https?://\\S+', user_input)\n            for url in urls:\n                if process_web_content(url):\n                    user_input = user_input.replace(url, '').strip()\n\n            # Process remaining input\n            user_input = re.sub(r'\\s+', ' ', user_input).strip()\n            if not user_input:\n                continue\n                \n            response = chat_agent.invoke(\n                {\"input\": user_input},\n                {\"configurable\": {\"session_id\": SESSION_ID}}\n            )\n            print(f\"\\nAI: {response['response']}\")\n            \n        except Exception as e:\n            print(f\"Error: {str(e)}\")\n            continue\n    \n    return chat_agent\n\ndefined_llm(OPENAI_API_KEY, chat_prompt=chat_prompt)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:40:23.034279Z","iopub.execute_input":"2025-02-23T20:40:23.034594Z","iopub.status.idle":"2025-02-23T20:42:21.656620Z","shell.execute_reply.started":"2025-02-23T20:40:23.034571Z","shell.execute_reply":"2025-02-23T20:42:21.655774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" 20 bees are sitting on 20 daisies, one bee on each flower. The flowers are arranged in a ring. From time to time 2 bees simultaneously fly  in opposite directions (clockwise and counterclockwise), each to its neighboring flower. Can all bees gather on the same daisy at some point? Will your answer be the same for 19 daisies and 19 bees?","metadata":{}},{"cell_type":"code","source":"\n\ndef defined_llm(OPENAI_API_KEY, chat_prompt=chat_prompt):\n    SESSION_ID = str(uuid4())\n    print(f\"Session ID: {SESSION_ID}\")\n    \n    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n    embedding_provider = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=800,\n        chunk_overlap=150,\n        separators=[r'\\n\\s*‚Ä¢', r'\\n\\d+\\.', r'\\n\\*', '\\n\\n'],\n        add_start_index=True\n    )\n    \n    vectorstore = FAISS.from_texts(\n        texts=[\"Math Knowledge Base Initialized\"], \n        embedding=embedding_provider,\n        metadatas=[{\"source\": \"system-init\"}]\n    )\n    retriever = vectorstore.as_retriever(\n        search_type=\"mmr\",\n        search_kwargs={\n            \"k\": 5,\n            \"score_threshold\": 0.7,\n            \"lambda_mult\": 0.5\n        }\n)\n\n    def format_docs(docs):\n        formatted = []\n        for i, doc in enumerate(docs):\n            # Handle Document objects\n            if hasattr(doc, 'metadata'):\n                source_type = doc.metadata.get('source_type', 'unknown')\n                source = doc.metadata.get('source_path', doc.metadata.get('source_url', 'unknown'))\n                content = doc.page_content\n            # Handle strings (fallback)\n            else:\n                source_type = \"unknown\"\n                source = \"unknown\"\n                content = str(doc)\n                \n            formatted.append(\n                f\"üìö Source {i+1} ({source_type}): {source}\\n\"\n                f\"{content[:500]}...\"\n            )\n        return \"\\n\\n\".join(formatted)\n\n    def debug_retrieval(query, retriever, top_k=3):\n        docs = retriever.invoke(query)\n        print(\"\\nüîç Retrieved Context Preview:\")\n        for i, doc in enumerate(docs[:top_k]):\n            print(f\"\\nüìÑ Document {i+1}:\")\n            print(f\"   Source: {doc.metadata.get('source_url', doc.metadata.get('source_path', 'unknown'))}\")\n            print(f\"   Content: {doc.page_content[:300]}...\")\n        return docs\n\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", chat_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"Context:\\n{context}\\n\\nQuestion: {input}\"),\n    ])\n\n    # Core processing chain\n    rag_chain = (\n        RunnablePassthrough.assign(\n            context=lambda x: format_docs(x[\"context\"]),\n            chat_history=lambda x: x[\"chat_history\"]\n        )\n        | prompt\n        | llm\n        | StrOutputParser()\n    )\n\n    contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question:\n    1. DO NOT modify or rephrase the original question\n    2. Instead, add any relevant background information from the chat history as a prefix\n    3. Format the output as:\n       [Background Info (if any)]\n       Original Question: <exact original question>\"\"\"\n    \n    contextualize_q_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", contextualize_q_system_prompt),\n        MessagesPlaceholder(\"chat_history\"),\n        (\"human\", \"{input}\"),\n    ])\n\n    def process_query(input_text, chat_history):\n        standalone_chain = contextualize_q_prompt | llm | StrOutputParser()\n        standalone_question = standalone_chain.invoke({\n            \"chat_history\": chat_history,\n            \"input\": input_text\n        })\n        \n        relevant_docs = retriever.invoke(standalone_question)\n        context = format_docs(relevant_docs)\n        debug_retrieval(standalone_question, retriever)\n        \n        response = rag_chain.invoke({\n            \"input\": input_text,\n            \"context\": context,\n            \"chat_history\": chat_history\n        })\n       \n        # Self-check verification\n        check_prompt = f\"\"\"Verify this solution contains:\n        1. Make sure the logic is correct. Check the method, formulas, each step.\n        2. Try to prove the response is incorrect. If it is incorrect, follow the logic and redo the problem.\n        3. Prove the response is correct.\n\n        If any issues are found, provide the correction.\n        If no issues are found, respond with \"VERIFIED\" and briefly explain why.\n        \n        Solution to verify:\n        {response}\n        \n        Missing/incorrect components:\"\"\"\n        \n        verification = rag_chain.invoke({\n            \"input\": check_prompt,\n            \"context\": context,\n            \"chat_history\": chat_history\n        })\n        \n        # If verification finds issues, get corrected solution\n        if \"VERIFIED\" not in verification:\n            corrected = rag_chain.invoke({\n                \"input\": f\"\"\"The previous solution had issues. Please provide a corrected solution that addresses these verification issues:{verification}\n                Original question: {input_text}\"\"\",\n                \"context\": context,\n                \"chat_history\": chat_history\n            })\n            return f\"{response}\\n\\n**Original Verification Feedback:**\\n{verification}\\n\\n**Corrected Response:**\\n{corrected}\"\n        \n        return f\"{response}\\n\\n**Verification:**\\n{verification}\"\n\n    # Memory\n    store = {}\n    def get_memory(session_id: str) -> ChatMessageHistory:\n        if session_id not in store:\n            store[session_id] = ChatMessageHistory(max_messages=20)\n        return store[session_id]\n\n    chat_agent = RunnableWithMessageHistory(\n        RunnablePassthrough.assign(\n            response=lambda x: process_query(x[\"input\"], x[\"chat_history\"])\n        ),\n        get_memory,\n        input_messages_key=\"input\",\n        history_messages_key=\"chat_history\",\n    )\n    \n    def process_file(file_path):\n        try:\n            if not os.path.exists(file_path):\n                raise FileNotFoundError(f\"File not found: {file_path}\")\n            \n            loader = PyPDFLoader(file_path) if file_path.endswith('.pdf') else TextLoader(file_path)\n            docs = loader.load()\n            splits = text_splitter.split_documents(docs)\n            \n            # Add metadata with content type detection\n            for split in splits:\n                split.metadata.update({\n                    \"source_type\": \"file\",\n                    \"source_path\": file_path,\n                    \"content_type\": self_detect_content_type(split.page_content)  # Added content type\n                })\n            \n            vectorstore.add_documents(splits)\n            print(f\"\\nAI: Added {len(splits)} chunks from {os.path.basename(file_path)}\")\n            return True\n        except Exception as e:\n            print(f\"\\nAI: Error processing {file_path}: {str(e)}\")\n            return False\n    \n    def process_web_content(url):\n        try:\n            loader = WebBaseLoader(\n                web_paths=[url],\n                bs_kwargs=dict(\n                    parse_only=bs4.SoupStrainer(\n                        # Universal content detection\n                        ['article', 'main', 'div', 'section', 'content'],\n                        class_=lambda value: value and any(\n                            kw in value.lower()\n                            for kw in ['content', 'article', 'main', 'body', 'text']\n                        )\n                    )\n                )\n            )\n            docs = loader.load()\n            splits = text_splitter.split_documents(docs)\n            \n            for split in splits:\n                split.metadata.update({\n                    \"source_type\": \"web\",\n                    \"source_url\": url,\n                    \"content_type\": self_detect_content_type(split.page_content)\n                })\n            \n            vectorstore.add_documents(splits)\n            print(f\"\\nAI: Added {len(splits)} chunks from {url}\")\n            return True\n        except Exception as e:\n            print(f\"\\nAI: Error processing {url}: {str(e)}\")\n            return False\n\n    def self_detect_content_type(text):\n        math_keywords = ['theorem', 'formula', 'equation', 'proof', 'lemma']\n        if any(kw in text.lower() for kw in math_keywords):\n            return \"math\"\n        return \"general\"\n\n    # Interactive loop\n    print(\"Math Expert System - Type 'exit' to quit\")\n    print(\"Input formats:\")\n    print(\"- RAG_file_path=\\\"/path/to/file.pdf\\\" https://example.com Your question\")\n    print(\"- Include math formulas using $...$ notation\")\n    \n    url_pattern = re.compile(\n        r'(?:http|ftp)s?://(?:[A-Z0-9-]+\\.)+[A-Z]{2,}(?::\\d+)?(?:/[\\w\\-./?%&=]*)?', \n        re.IGNORECASE\n    )\n    \n    while True:\n        try:\n            user_input = input(\"\\nYou: \").strip()\n            if user_input.lower() in ['exit', 'quit']:\n                print(\"Goodbye!\")\n                break\n\n            # Process file paths\n            file_paths = re.findall(r'RAG_file_path=\"([^\"]+)\"', user_input)\n            for fp in file_paths:\n                if process_file(fp):\n                    user_input = user_input.replace(f'RAG_file_path=\"{fp}\"', '').strip()\n\n            # Process URLs\n            urls = re.findall(r'https?://\\S+', user_input)\n            for url in urls:\n                if process_web_content(url):\n                    user_input = user_input.replace(url, '').strip()\n\n            # Process remaining input\n            user_input = re.sub(r'\\s+', ' ', user_input).strip()\n            if not user_input:\n                continue\n                \n            response = chat_agent.invoke(\n                {\"input\": user_input},\n                {\"configurable\": {\"session_id\": SESSION_ID}}\n            )\n            print(f\"\\nAI: {response['response']}\")\n            \n        except Exception as e:\n            print(f\"Error: {str(e)}\")\n            continue\n    \n    return chat_agent\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:46:24.660215Z","iopub.execute_input":"2025-02-23T20:46:24.660685Z","iopub.status.idle":"2025-02-23T20:46:24.690675Z","shell.execute_reply.started":"2025-02-23T20:46:24.660640Z","shell.execute_reply":"2025-02-23T20:46:24.689499Z"}},"outputs":[],"execution_count":null}]}